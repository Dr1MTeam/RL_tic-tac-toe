{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets # Для игры\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeAgent, self).__init__()\n",
    "        # Сверточные слои для обработки состояния в виде матрицы\n",
    "        N = 32\n",
    "        self.conv1 = nn.Conv2d(1, N, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(N)\n",
    "        self.conv2 = nn.Conv2d(N, 2*N, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(2*N)\n",
    "        self.conv3 = nn.Conv2d(2*N, 4*N, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(4*N)\n",
    "        # Финальный сверточный слой для получения выходного канала с вероятностями\n",
    "        self.conv4 = nn.Conv2d(4*N, 1, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.player = 1\n",
    "\n",
    "        self.start_epsilon = 1.\n",
    "        self.end_epsilon = 0.01\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mask = (x != 0).float()\n",
    "        \n",
    "        x = x * self.player\n",
    "\n",
    "        # Первый сверточный блок\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Второй сверточный блок\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Третий сверточный блок\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Выходной слой\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        # Применяем маску, обнуляя выходные значения на занятых клетках\n",
    "        x = x.masked_fill(mask == 1, float('-inf'))\n",
    "        \n",
    "        # Применяем softmax для получения вероятностей\n",
    "        batch_size, _, height, width = x.shape\n",
    "        x = x.view(batch_size, -1)  # Преобразуем в [batch_size, num_fields]\n",
    "        x = x - x.max(dim=1, keepdim=True).values\n",
    "        x = F.softmax(x, dim=1)     # Применяем softmax по полям\n",
    "        x = x.view(batch_size, height, width)  # Восстанавливаем матрицу вероятностей\n",
    "        \n",
    "        return x  # Выходная матрица вероятностей\n",
    "    def select_move(self, probabilities):\n",
    "        # Преобразуем вероятности в одномерный вид\n",
    "        batch_size, height, width = probabilities.shape\n",
    "        probabilities = probabilities.view(batch_size, -1)  # [batch_size, 9]\n",
    "        \n",
    "        # Для каждого элемента в batch выбираем ход на основе вероятностей\n",
    "        distribution = Categorical(probabilities)\n",
    "        move_index = distribution.sample()  # Индекс выбранной ячейки\n",
    "        \n",
    "        # Преобразуем индекс в координаты строки и столбца\n",
    "        row, col = divmod(move_index.item(), width)\n",
    "        \n",
    "        return row, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.3975],\n",
      "         [0.0000, 0.0000, 0.4038],\n",
      "         [0.0000, 0.0000, 0.1987]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 3, 3])\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "agent = TicTacToeAgent()\n",
    "input_tensor = torch.tensor([[[[-1., -1., 0.], [1., 1., 0.], [1., 1., 0.]]]])\n",
    "# input_tensor = torch.zeros(1, 1, 3, 3)  # Входная матрица 3x3\n",
    "output = agent(input_tensor)\n",
    "print(output)\n",
    "print(output.shape)  # Ожидаемый результат: [1, 3, 3] для 3x3 входа\n",
    "print(agent.select_move(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "    def __init__(self, size = 3, device = 'cuda'):\n",
    "        self.size = size\n",
    "        self.board = torch.zeros(self.size, self.size, dtype=torch.float32).to(device=device)\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = torch.zeros(self.size, self.size, dtype=torch.float32).to(device=self.device)\n",
    "        self.current_player = 1\n",
    "        return self.board.unsqueeze(0).unsqueeze(0)  # [1, 1, 3, 3]\n",
    "    \n",
    "    def get_board(self):\n",
    "        return self.board\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        valid_actions = (self.board == 0).nonzero(as_tuple=True)  # Получаем индексы доступных клеток\n",
    "        return list(zip(valid_actions[0].tolist(), valid_actions[1].tolist()))\n",
    "\n",
    "\n",
    "    def step(self, action, player):\n",
    "        row, col = action\n",
    "        \n",
    "        if self.board[row, col] != 0:\n",
    "            raise ValueError(\"Invalid move\")\n",
    "        \n",
    "        self.board[row, col] = player\n",
    "        reward, done = self.check_winner33()\n",
    "        return self.board.unsqueeze(0).unsqueeze(0), reward * player, done\n",
    "\n",
    "    def check_winner33(self):\n",
    "        for i in range(3):\n",
    "            if abs(self.board[i, :].sum()) == 3 or abs(self.board[:, i].sum()) == 3:\n",
    "                return 1, True\n",
    "        if abs(self.board.trace()) == 3 or abs(torch.fliplr(self.board).trace()) == 3:\n",
    "            return 1, True\n",
    "        if (self.board == 0).sum() == 0:\n",
    "            return 0, True\n",
    "        return 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Победа игрока -1\n"
     ]
    }
   ],
   "source": [
    "def check_victory(board):\n",
    "    \"\"\"\n",
    "    Проверяет, есть ли победа для одного из игроков на доске.\n",
    "    \n",
    "    Аргументы:\n",
    "        board (torch.Tensor): тензор размером (1, 1, N, N) с элементами -1, 0, 1.\n",
    "        \n",
    "    Возвращает:\n",
    "        int: 1, если победа игрока 1; -1, если победа игрока -1; 0, если победителей нет.\n",
    "    \"\"\"\n",
    "    board = board.squeeze()  # Удаляем размерности (1, 1, N, N) -> (N, N)\n",
    "    N = board.size(0)\n",
    "\n",
    "    # Проверяем строки и столбцы на наличие тройки подряд\n",
    "    for i in range(N):\n",
    "        for j in range(N - 2):  # Проходим по всем возможным начальным индексам тройки\n",
    "            # Проверка строки\n",
    "            if board[i, j] == board[i, j + 1] == board[i, j + 2] and board[i, j] != 0:\n",
    "                return board[i, j]\n",
    "            # Проверка столбца\n",
    "            if board[j, i] == board[j + 1, i] == board[j + 2, i] and board[j, i] != 0:\n",
    "                return board[j, i]\n",
    "    \n",
    "    # Проверяем диагонали на наличие тройки подряд\n",
    "    for i in range(N - 2):\n",
    "        for j in range(N - 2):\n",
    "            # Проверка главной диагонали\n",
    "            if board[i, j] == board[i + 1, j + 1] == board[i + 2, j + 2] and board[i, j] != 0:\n",
    "                return board[i, j]\n",
    "            # Проверка побочной диагонали\n",
    "            if board[i, j + 2] == board[i + 1, j + 1] == board[i + 2, j] and board[i, j + 2] != 0:\n",
    "                return board[i, j + 2]\n",
    "    \n",
    "    # Если нет победителя\n",
    "    return 0\n",
    "\n",
    "# Пример использования\n",
    "board = torch.tensor([[[[0, 1, -1],\n",
    "                        [-1, -1, 1],\n",
    "                        [-1, 0, 0]\n",
    "                        ]]])  # Размер (1, 1, 4, 4)\n",
    "\n",
    "result = check_victory(board)\n",
    "if result == 1:\n",
    "    print(\"Победа игрока 1\")\n",
    "elif result == -1:\n",
    "    print(\"Победа игрока -1\")\n",
    "else:\n",
    "    print(\"Ничья\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 94273\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Пример использования с вашей моделью\n",
    "\n",
    "print(f\"Количество обучаемых параметров: {count_parameters(agent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_eval(agent, env, num_games=100):\n",
    "    \"\"\"\n",
    "    Оценка процента побед агента:\n",
    "    - Против случайного игрока.\n",
    "    :param agent: Агент, который делает предсказания ходов.\n",
    "    :param env: Среда игры.\n",
    "    :param num_games: Количество игр для оценки.\n",
    "    :return: Словарь с процентами побед.\n",
    "    \"\"\"\n",
    "    def play_game(agent):\n",
    "        \"\"\"Запуск одной игры между двумя агентами.\"\"\"\n",
    "        env.reset()\n",
    "        done = False\n",
    "        player = 1\n",
    "\n",
    "        while not done:\n",
    "            state = env.board.unsqueeze(0).unsqueeze(0) * player\n",
    "            if player == 1:\n",
    "                probabilities = agent(state)\n",
    "                row, col = agent.select_move(probabilities) # Здесь по идее надо поменять на аргмакс\n",
    "            else:\n",
    "                # Если agent2 — случайный игрок, выбираем случайный ход\n",
    "                valid_actions = env.get_valid_actions()\n",
    "                if valid_actions:\n",
    "                    row, col = random.choice(valid_actions)\n",
    "                \n",
    "\n",
    "            # Применяем ход\n",
    "            _, reward, done = env.step((row, col), player)\n",
    "            player *= -1\n",
    "\n",
    "        return reward  # 1 = победа первого игрока, -1 = победа второго игрока, 0 = ничья\n",
    "\n",
    "    def evaluate(agent):\n",
    "        \"\"\"Оценка процента побед первого агента.\"\"\"\n",
    "        results = {\"agent1_wins\": 0, \"agent2_wins\": 0, \"draws\": 0}\n",
    "\n",
    "        for _ in range(num_games):\n",
    "            winner = play_game(agent)\n",
    "            if winner == 1:\n",
    "                results[\"agent1_wins\"] += 1\n",
    "            elif winner == -1:\n",
    "                results[\"agent2_wins\"] += 1\n",
    "            else:\n",
    "                results[\"draws\"] += 1\n",
    "\n",
    "        total_games = num_games\n",
    "        results[\"agent1_win_rate\"] = results[\"agent1_wins\"] / total_games * 100\n",
    "        results[\"agent2_win_rate\"] = results[\"agent2_wins\"] / total_games * 100\n",
    "        results[\"draw_rate\"] = results[\"draws\"] / total_games * 100\n",
    "        return results\n",
    "\n",
    "\n",
    "    # Оценка против случайного игрока\n",
    "    print(\"Evaluating against random player...\")\n",
    "    with torch.no_grad():\n",
    "        random_play_results = evaluate(agent.eval())\n",
    "    agent.train()\n",
    "    return {\n",
    "        \"random_play\": random_play_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating against random player...\n",
      "episode=0, {'random_play': {'agent1_wins': 60, 'agent2_wins': 29, 'draws': 11, 'agent1_win_rate': 60.0, 'agent2_win_rate': 28.999999999999996, 'draw_rate': 11.0}}\n",
      "loss=tensor(0., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=1000, {'random_play': {'agent1_wins': 58, 'agent2_wins': 28, 'draws': 14, 'agent1_win_rate': 57.99999999999999, 'agent2_win_rate': 28.000000000000004, 'draw_rate': 14.000000000000002}}\n",
      "loss=tensor(7.5285, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=2000, {'random_play': {'agent1_wins': 66, 'agent2_wins': 16, 'draws': 18, 'agent1_win_rate': 66.0, 'agent2_win_rate': 16.0, 'draw_rate': 18.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=3000, {'random_play': {'agent1_wins': 70, 'agent2_wins': 24, 'draws': 6, 'agent1_win_rate': 70.0, 'agent2_win_rate': 24.0, 'draw_rate': 6.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=4000, {'random_play': {'agent1_wins': 55, 'agent2_wins': 27, 'draws': 18, 'agent1_win_rate': 55.00000000000001, 'agent2_win_rate': 27.0, 'draw_rate': 18.0}}\n",
      "loss=tensor(9.3023, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=5000, {'random_play': {'agent1_wins': 64, 'agent2_wins': 20, 'draws': 16, 'agent1_win_rate': 64.0, 'agent2_win_rate': 20.0, 'draw_rate': 16.0}}\n",
      "loss=tensor(-4.6389, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=6000, {'random_play': {'agent1_wins': 77, 'agent2_wins': 17, 'draws': 6, 'agent1_win_rate': 77.0, 'agent2_win_rate': 17.0, 'draw_rate': 6.0}}\n",
      "loss=tensor(4.4560, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=7000, {'random_play': {'agent1_wins': 65, 'agent2_wins': 17, 'draws': 18, 'agent1_win_rate': 65.0, 'agent2_win_rate': 17.0, 'draw_rate': 18.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=8000, {'random_play': {'agent1_wins': 75, 'agent2_wins': 20, 'draws': 5, 'agent1_win_rate': 75.0, 'agent2_win_rate': 20.0, 'draw_rate': 5.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=9000, {'random_play': {'agent1_wins': 70, 'agent2_wins': 17, 'draws': 13, 'agent1_win_rate': 70.0, 'agent2_win_rate': 17.0, 'draw_rate': 13.0}}\n",
      "loss=tensor(0., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=10000, {'random_play': {'agent1_wins': 78, 'agent2_wins': 16, 'draws': 6, 'agent1_win_rate': 78.0, 'agent2_win_rate': 16.0, 'draw_rate': 6.0}}\n",
      "loss=tensor(9.6528, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=11000, {'random_play': {'agent1_wins': 72, 'agent2_wins': 11, 'draws': 17, 'agent1_win_rate': 72.0, 'agent2_win_rate': 11.0, 'draw_rate': 17.0}}\n",
      "loss=tensor(-9.8371, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=12000, {'random_play': {'agent1_wins': 65, 'agent2_wins': 16, 'draws': 19, 'agent1_win_rate': 65.0, 'agent2_win_rate': 16.0, 'draw_rate': 19.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=13000, {'random_play': {'agent1_wins': 78, 'agent2_wins': 18, 'draws': 4, 'agent1_win_rate': 78.0, 'agent2_win_rate': 18.0, 'draw_rate': 4.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=14000, {'random_play': {'agent1_wins': 73, 'agent2_wins': 16, 'draws': 11, 'agent1_win_rate': 73.0, 'agent2_win_rate': 16.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-7.4142, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=15000, {'random_play': {'agent1_wins': 69, 'agent2_wins': 18, 'draws': 13, 'agent1_win_rate': 69.0, 'agent2_win_rate': 18.0, 'draw_rate': 13.0}}\n",
      "loss=tensor(8.6523, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=16000, {'random_play': {'agent1_wins': 82, 'agent2_wins': 9, 'draws': 9, 'agent1_win_rate': 82.0, 'agent2_win_rate': 9.0, 'draw_rate': 9.0}}\n",
      "loss=tensor(10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=17000, {'random_play': {'agent1_wins': 81, 'agent2_wins': 12, 'draws': 7, 'agent1_win_rate': 81.0, 'agent2_win_rate': 12.0, 'draw_rate': 7.000000000000001}}\n",
      "loss=tensor(10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=18000, {'random_play': {'agent1_wins': 62, 'agent2_wins': 17, 'draws': 21, 'agent1_win_rate': 62.0, 'agent2_win_rate': 17.0, 'draw_rate': 21.0}}\n",
      "loss=tensor(-5.6757, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=19000, {'random_play': {'agent1_wins': 76, 'agent2_wins': 11, 'draws': 13, 'agent1_win_rate': 76.0, 'agent2_win_rate': 11.0, 'draw_rate': 13.0}}\n",
      "loss=tensor(-8.5982, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=20000, {'random_play': {'agent1_wins': 78, 'agent2_wins': 10, 'draws': 12, 'agent1_win_rate': 78.0, 'agent2_win_rate': 10.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(8.4135, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=21000, {'random_play': {'agent1_wins': 71, 'agent2_wins': 16, 'draws': 13, 'agent1_win_rate': 71.0, 'agent2_win_rate': 16.0, 'draw_rate': 13.0}}\n",
      "loss=tensor(10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=22000, {'random_play': {'agent1_wins': 78, 'agent2_wins': 11, 'draws': 11, 'agent1_win_rate': 78.0, 'agent2_win_rate': 11.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-3.1852, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=23000, {'random_play': {'agent1_wins': 67, 'agent2_wins': 21, 'draws': 12, 'agent1_win_rate': 67.0, 'agent2_win_rate': 21.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=24000, {'random_play': {'agent1_wins': 75, 'agent2_wins': 13, 'draws': 12, 'agent1_win_rate': 75.0, 'agent2_win_rate': 13.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(-3.2258, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=25000, {'random_play': {'agent1_wins': 80, 'agent2_wins': 6, 'draws': 14, 'agent1_win_rate': 80.0, 'agent2_win_rate': 6.0, 'draw_rate': 14.000000000000002}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=26000, {'random_play': {'agent1_wins': 74, 'agent2_wins': 13, 'draws': 13, 'agent1_win_rate': 74.0, 'agent2_win_rate': 13.0, 'draw_rate': 13.0}}\n",
      "loss=tensor(0., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=27000, {'random_play': {'agent1_wins': 80, 'agent2_wins': 14, 'draws': 6, 'agent1_win_rate': 80.0, 'agent2_win_rate': 14.000000000000002, 'draw_rate': 6.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=28000, {'random_play': {'agent1_wins': 87, 'agent2_wins': 5, 'draws': 8, 'agent1_win_rate': 87.0, 'agent2_win_rate': 5.0, 'draw_rate': 8.0}}\n",
      "loss=tensor(4.6938, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=29000, {'random_play': {'agent1_wins': 74, 'agent2_wins': 12, 'draws': 14, 'agent1_win_rate': 74.0, 'agent2_win_rate': 12.0, 'draw_rate': 14.000000000000002}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=30000, {'random_play': {'agent1_wins': 75, 'agent2_wins': 11, 'draws': 14, 'agent1_win_rate': 75.0, 'agent2_win_rate': 11.0, 'draw_rate': 14.000000000000002}}\n",
      "loss=tensor(-1.7512, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=31000, {'random_play': {'agent1_wins': 81, 'agent2_wins': 8, 'draws': 11, 'agent1_win_rate': 81.0, 'agent2_win_rate': 8.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-1.1550, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=32000, {'random_play': {'agent1_wins': 85, 'agent2_wins': 4, 'draws': 11, 'agent1_win_rate': 85.0, 'agent2_win_rate': 4.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=33000, {'random_play': {'agent1_wins': 87, 'agent2_wins': 1, 'draws': 12, 'agent1_win_rate': 87.0, 'agent2_win_rate': 1.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(2.7462, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=34000, {'random_play': {'agent1_wins': 81, 'agent2_wins': 13, 'draws': 6, 'agent1_win_rate': 81.0, 'agent2_win_rate': 13.0, 'draw_rate': 6.0}}\n",
      "loss=tensor(8.1618, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=35000, {'random_play': {'agent1_wins': 74, 'agent2_wins': 17, 'draws': 9, 'agent1_win_rate': 74.0, 'agent2_win_rate': 17.0, 'draw_rate': 9.0}}\n",
      "loss=tensor(-6.3810, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=36000, {'random_play': {'agent1_wins': 77, 'agent2_wins': 11, 'draws': 12, 'agent1_win_rate': 77.0, 'agent2_win_rate': 11.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=37000, {'random_play': {'agent1_wins': 84, 'agent2_wins': 8, 'draws': 8, 'agent1_win_rate': 84.0, 'agent2_win_rate': 8.0, 'draw_rate': 8.0}}\n",
      "loss=tensor(3.1500, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=38000, {'random_play': {'agent1_wins': 77, 'agent2_wins': 11, 'draws': 12, 'agent1_win_rate': 77.0, 'agent2_win_rate': 11.0, 'draw_rate': 12.0}}\n",
      "loss=tensor(10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=39000, {'random_play': {'agent1_wins': 80, 'agent2_wins': 10, 'draws': 10, 'agent1_win_rate': 80.0, 'agent2_win_rate': 10.0, 'draw_rate': 10.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=40000, {'random_play': {'agent1_wins': 83, 'agent2_wins': 11, 'draws': 6, 'agent1_win_rate': 83.0, 'agent2_win_rate': 11.0, 'draw_rate': 6.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=41000, {'random_play': {'agent1_wins': 80, 'agent2_wins': 6, 'draws': 14, 'agent1_win_rate': 80.0, 'agent2_win_rate': 6.0, 'draw_rate': 14.000000000000002}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=42000, {'random_play': {'agent1_wins': 81, 'agent2_wins': 7, 'draws': 12, 'agent1_win_rate': 81.0, 'agent2_win_rate': 7.000000000000001, 'draw_rate': 12.0}}\n",
      "loss=tensor(5.8743, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=43000, {'random_play': {'agent1_wins': 81, 'agent2_wins': 8, 'draws': 11, 'agent1_win_rate': 81.0, 'agent2_win_rate': 8.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-7.6448, device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=44000, {'random_play': {'agent1_wins': 85, 'agent2_wins': 10, 'draws': 5, 'agent1_win_rate': 85.0, 'agent2_win_rate': 10.0, 'draw_rate': 5.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=45000, {'random_play': {'agent1_wins': 78, 'agent2_wins': 11, 'draws': 11, 'agent1_win_rate': 78.0, 'agent2_win_rate': 11.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=46000, {'random_play': {'agent1_wins': 73, 'agent2_wins': 16, 'draws': 11, 'agent1_win_rate': 73.0, 'agent2_win_rate': 16.0, 'draw_rate': 11.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=47000, {'random_play': {'agent1_wins': 79, 'agent2_wins': 13, 'draws': 8, 'agent1_win_rate': 79.0, 'agent2_win_rate': 13.0, 'draw_rate': 8.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=48000, {'random_play': {'agent1_wins': 90, 'agent2_wins': 7, 'draws': 3, 'agent1_win_rate': 90.0, 'agent2_win_rate': 7.000000000000001, 'draw_rate': 3.0}}\n",
      "loss=tensor(-10., device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "Evaluating against random player...\n",
      "episode=49000, {'random_play': {'agent1_wins': 84, 'agent2_wins': 7, 'draws': 9, 'agent1_win_rate': 84.0, 'agent2_win_rate': 7.000000000000001, 'draw_rate': 9.0}}\n",
      "loss=tensor(4.7775, device='cuda:0', grad_fn=<ClampBackward1>)\n"
     ]
    }
   ],
   "source": [
    "## 1 версия\n",
    "\n",
    "agent = TicTacToeAgent().to(device=DEVICE)\n",
    "optimizer = optim.AdamW(agent.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "env = TicTacToeEnv(device=DEVICE)\n",
    "\n",
    "num_episodes = 50000\n",
    "gamma = 0.99  # Коэффициент дисконтирования\n",
    "\n",
    "eps = 0.5\n",
    "eps_min = 0.05\n",
    "decay = 0.9999\n",
    "\n",
    "vals = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Получение вероятностей для действий\n",
    "        probabilities = agent(state)\n",
    "        if random.random() < eps:\n",
    "            actions = env.get_valid_actions()\n",
    "            if actions:\n",
    "                row, col = random.choice(actions)\n",
    "        else:\n",
    "            row, col = agent.select_move(probabilities)\n",
    "\n",
    "        # Выполнение действия в среде\n",
    "        next_state, reward, done = env.step((row, col), agent.player)\n",
    "\n",
    "        # Сохранение логарифма вероятности выбранного действия\n",
    "        log_prob = torch.log(probabilities[0, row, col])\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Сохранение вознаграждения\n",
    "        rewards.append(reward * agent.player)\n",
    "        \n",
    "        agent.player *=-1\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    G = 0\n",
    "    player = 1\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        G = G * player\n",
    "        player *=-1\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # Нормализация\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    # Вычисление функции потерь\n",
    "    loss = torch.stack([-log_prob * G for log_prob, G in zip(log_probs, returns)]).sum()\n",
    "    loss = loss.clamp(min=-10, max=10)\n",
    "    # Обновление параметров модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    eps = max(eps_min, eps*decay)\n",
    "\n",
    "    if episode % 1000 == 0:\n",
    "        eval = agent_eval(agent=agent, env=env, num_games=100)\n",
    "        print(f\"{episode=}, {eval}\")\n",
    "        print(f\"{loss=}\")\n",
    "        vals.append(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), \"agent_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag = TicTacToeAgent()\n",
    "ag.load_state_dict(torch.load(\"agent_weights.pth\", weights_only = True))\n",
    "ag.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating against random player...\n",
      "{'random_play': {'agent1_wins': 93, 'agent2_wins': 0, 'draws': 7, 'agent1_win_rate': 93.0, 'agent2_win_rate': 0.0, 'draw_rate': 7.000000000000001}}\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToeEnv(device=DEVICE)\n",
    "env.reset()\n",
    "# Оценка агента\n",
    "results = agent_eval(agent, env)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2427f7f208354addb07e04a57a1ab005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Button(description=' ', style=ButtonStyle()), Button(description=' ', style=ButtonStyle()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dbc99e8a5b4c869f9eca8eba100b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Restart', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418a32d5d62a4533857cc9553329844a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TicTacToeWithAgent at 0x1d0f1c4c650>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TicTacToeWithAgent:\n",
    "    def __init__(self, agent, check_winner_func, board_size=3, device = 'cuda'):\n",
    "        self.board_size = board_size\n",
    "        self.device = device\n",
    "        self.board = torch.zeros((1, 1, board_size, board_size), dtype=torch.float32).to(device=self.device )\n",
    "        self.current_player = 1  # Игрок 1 - человек, Игрок -1 - агент\n",
    "        self.agent = agent  # Функция агента\n",
    "        self.check_winner = check_winner_func  # Функция проверки победы\n",
    "        self.buttons = [widgets.Button(description=\" \") for _ in range(board_size * board_size)]\n",
    "        self.output = widgets.Output()\n",
    "        self.restart_button = widgets.Button(description=\"Restart\", button_style=\"danger\")\n",
    "        self.create_ui()\n",
    "\n",
    "    def create_ui(self):\n",
    "        grid = widgets.GridBox(\n",
    "            children=self.buttons,\n",
    "            layout=widgets.Layout(\n",
    "                grid_template_columns=f\"repeat({self.board_size}, 100px)\",\n",
    "                grid_template_rows=f\"repeat({self.board_size}, 100px)\",\n",
    "                grid_gap=\"5px\",\n",
    "            ),\n",
    "        )\n",
    "        for i, button in enumerate(self.buttons):\n",
    "            button.on_click(lambda b, i=i: self.make_human_move(i))\n",
    "        \n",
    "        self.restart_button.on_click(lambda b: self.restart_game())\n",
    "        display(grid, self.restart_button, self.output)\n",
    "\n",
    "    def make_human_move(self, index):\n",
    "        x, y = divmod(index, self.board_size)\n",
    "        if self.board[0, 0, x, y] == 0 and self.current_player == 1:\n",
    "            self.board[0, 0, x, y] = self.current_player\n",
    "            self.update_button(index, self.current_player)\n",
    "            self.check_game_status()\n",
    "\n",
    "            if self.current_player == -1:  # Если игра продолжается, ход агента\n",
    "                self.make_agent_move()\n",
    "\n",
    "    def make_agent_move(self):\n",
    "        self.agent.player = -1\n",
    "        probs = self.agent(self.board)\n",
    "        # row, col = self.agent.select_move(probs)  # Агент выбирает оптимальный ход\n",
    "        flat_index = torch.argmax(probs).item()\n",
    "        row, col = divmod(flat_index, probs.shape[1])\n",
    "        if self.board[0, 0, row, col] == 0:  # Если клетка пуста\n",
    "            self.board[0, 0, row, col] = -1  # Агент делает ход (-1)\n",
    "            self.update_ui_from_board()  # Обновляем интерфейс\n",
    "            self.check_game_status()  # Проверяем статус игры\n",
    "\n",
    "\n",
    "    def check_game_status(self):\n",
    "        winner = self.check_winner(self.board)\n",
    "        if winner != 0:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                if winner == 1:\n",
    "                    print(\"Player wins!\")\n",
    "                elif winner == -1:\n",
    "                    print(\"Agent wins!\")\n",
    "            self.disable_all_buttons()\n",
    "        elif (self.board == 0).sum().item() == 0:  # Ничья\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(\"It's a draw!\")\n",
    "        else:\n",
    "            self.current_player *= -1  # Смена игрока\n",
    "\n",
    "    def update_button(self, index, player):\n",
    "        self.buttons[index].description = \"X\" if player == 1 else \"O\"\n",
    "        self.buttons[index].disabled = True\n",
    "\n",
    "    def update_ui_from_board(self):\n",
    "        for i in range(self.board_size * self.board_size):\n",
    "            x, y = divmod(i, self.board_size)\n",
    "            value = self.board[0, 0, x, y].item()\n",
    "            if value == 1:\n",
    "                self.buttons[i].description = \"X\"\n",
    "                self.buttons[i].disabled = True\n",
    "            elif value == -1:\n",
    "                self.buttons[i].description = \"O\"\n",
    "                self.buttons[i].disabled = True\n",
    "            else:\n",
    "                self.buttons[i].description = \" \"\n",
    "                self.buttons[i].disabled = False\n",
    "\n",
    "    def disable_all_buttons(self):\n",
    "        for button in self.buttons:\n",
    "            button.disabled = True\n",
    "\n",
    "    def restart_game(self):\n",
    "        self.board = torch.zeros((1, 1, self.board_size, self.board_size), dtype=torch.float32).to(device=self.device )\n",
    "        self.current_player = 1\n",
    "        for button in self.buttons:\n",
    "            button.description = \" \"\n",
    "            button.disabled = False\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "\n",
    "\n",
    "# Пример функции проверки победы\n",
    "def check_winner_func(board):\n",
    "    # Аргумент board: тензор (1, 1, N, N)\n",
    "    board = board[0, 0]\n",
    "    N = board.shape[0]\n",
    "    for i in range(N):\n",
    "        if torch.abs(board[i, :].sum()) == N:  # Горизонтали\n",
    "            return board[i, 0].item()\n",
    "        if torch.abs(board[:, i].sum()) == N:  # Вертикали\n",
    "            return board[0, i].item()\n",
    "    if torch.abs(board.diag().sum()) == N:  # Главная диагональ\n",
    "        return board[0, 0].item()\n",
    "    if torch.abs(torch.flip(board, dims=[1]).diag().sum()) == N:  # Побочная диагональ\n",
    "        return board[0, N - 1].item()\n",
    "    return 0  # Победителя нет\n",
    "\n",
    "# Пример агента, который делает случайный ход\n",
    "def random_agent(board):\n",
    "    empty_positions = (board[0, 0] == 0).nonzero(as_tuple=False)\n",
    "    if len(empty_positions) > 0:\n",
    "        move = empty_positions[torch.randint(len(empty_positions), (1,))].tolist()\n",
    "        board[0, 0, move[0], move[1]] = -1  # Агент всегда играет -1\n",
    "    return board\n",
    "\n",
    "\n",
    "# Запуск игры\n",
    "TicTacToeWithAgent(agent=ag, check_winner_func=check_victory, board_size=3, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
