{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeAgent, self).__init__()\n",
    "        # Сверточные слои для обработки состояния в виде матрицы\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        # Финальный сверточный слой для получения выходного канала с вероятностями\n",
    "        self.conv4 = nn.Conv2d(16, 1, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.player = 1\n",
    "\n",
    "        self.start_epsilon = 1.\n",
    "        self.end_epsilon = 0.01\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mask = (x != 0).float()\n",
    "        \n",
    "        x = x * self.player\n",
    "\n",
    "        # Первый сверточный блок\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Второй сверточный блок\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Третий сверточный блок\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Выходной слой\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        # Применяем маску, обнуляя выходные значения на занятых клетках\n",
    "        x = x.masked_fill(mask == 1, float('-inf'))\n",
    "        \n",
    "        # Применяем softmax для получения вероятностей\n",
    "        batch_size, _, height, width = x.shape\n",
    "        x = x.view(batch_size, -1)  # Преобразуем в [batch_size, num_fields]\n",
    "        x = F.softmax(x, dim=1)     # Применяем softmax по полям\n",
    "        x = x.view(batch_size, height, width)  # Восстанавливаем матрицу вероятностей\n",
    "        \n",
    "        return x  # Выходная матрица вероятностей\n",
    "    def select_move(self, probabilities):\n",
    "        # Преобразуем вероятности в одномерный вид\n",
    "        batch_size, height, width = probabilities.shape\n",
    "        probabilities = probabilities.view(batch_size, -1)  # [batch_size, 9]\n",
    "        \n",
    "        # Для каждого элемента в batch выбираем ход на основе вероятностей\n",
    "        distribution = Categorical(probabilities)\n",
    "        move_index = distribution.sample()  # Индекс выбранной ячейки\n",
    "        \n",
    "        # Преобразуем индекс в координаты строки и столбца\n",
    "        row, col = divmod(move_index.item(), width)\n",
    "        \n",
    "        return row, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.1989],\n",
      "         [0.0000, 0.0000, 0.1779],\n",
      "         [0.0000, 0.0000, 0.3307],\n",
      "         [0.0000, 0.0000, 0.2925]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 4, 3])\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Пример использования\n",
    "agent = TicTacToeAgent()\n",
    "input_tensor = torch.tensor([[[[-1., -1., 0.], [1., 1., 0.], [1., 1., 0.]]]])\n",
    "# input_tensor = torch.zeros(1, 1, 3, 3)  # Входная матрица 3x3\n",
    "output = agent(input_tensor)\n",
    "print(output)\n",
    "print(output.shape)  # Ожидаемый результат: [1, 3, 3] для 3x3 входа\n",
    "print(agent.select_move(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.board = torch.zeros(3, 3, dtype=torch.float32)\n",
    "        self.current_player = 1  # 1 или -1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = torch.zeros(3, 3, dtype=torch.float32)\n",
    "        self.current_player = 1\n",
    "        return self.board.unsqueeze(0).unsqueeze(0)  # [1, 1, 3, 3]\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = action\n",
    "        if self.board[row, col] != 0:\n",
    "            raise ValueError(\"Invalid move\")\n",
    "        \n",
    "        self.board[row, col] = self.current_player\n",
    "        reward, done = self.check_winner()\n",
    "        self.current_player *= -1\n",
    "        return self.board.unsqueeze(0).unsqueeze(0), reward, done\n",
    "\n",
    "    def check_winner(self):\n",
    "        for i in range(3):\n",
    "            if abs(self.board[i, :].sum()) == 3 or abs(self.board[:, i].sum()) == 3:\n",
    "                return 1 * self.current_player, True\n",
    "        if abs(self.board.trace()) == 3 or abs(torch.fliplr(self.board).trace()) == 3:\n",
    "            return 1 * self.current_player, True\n",
    "        if (self.board == 0).sum() == 0:\n",
    "            return 0, True\n",
    "        return 0, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 1705\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Пример использования с вашей моделью\n",
    "\n",
    "print(f\"Количество обучаемых параметров: {count_parameters(agent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining hyperparameters for the training\n",
    "\n",
    "EPISODES = 4000000            # total no. of episodes\n",
    "LR = 0.01                     # learning rate\n",
    "GAMMA = 0.9                   # discount factor\n",
    "\n",
    "max_epsilon = 1.0             # Greed: 100%\n",
    "min_epsilon = 0.001           # Min_Greed: 0.1%\n",
    "decay_rate = 0.000002         # epsilon decay rate\n",
    "threshold = 2500              # no. of episodes after which states_tracked will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: 3.5547351837158203\n",
      "Episode 10000, Loss: 0.0\n",
      "Episode 20000, Loss: 0.5711522698402405\n",
      "Episode 30000, Loss: -0.16524383425712585\n",
      "Episode 40000, Loss: -0.9223109483718872\n",
      "Episode 50000, Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Получение вероятностей для действий\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m agent(state)\n\u001b[1;32m---> 17\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Выполнение действия в среде\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep((row, col))\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mTicTacToeAgent.select_move\u001b[1;34m(self, probabilities)\u001b[0m\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, height, width)  \u001b[38;5;66;03m# Восстанавливаем матрицу вероятностей\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x  \u001b[38;5;66;03m# Выходная матрица вероятностей\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_move\u001b[39m(\u001b[38;5;28mself\u001b[39m, probabilities):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Преобразуем вероятности в одномерный вид\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     batch_size, height, width \u001b[38;5;241m=\u001b[39m probabilities\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     48\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m probabilities\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 9]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 1 версия\n",
    "\n",
    "agent = TicTacToeAgent()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "num_episodes = 200000\n",
    "gamma = 0.99  # Коэффициент дисконтирования\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Получение вероятностей для действий\n",
    "        probabilities = agent(state)\n",
    "        row, col = agent.select_move(probabilities)\n",
    "\n",
    "        # Выполнение действия в среде\n",
    "        next_state, reward, done = env.step((row, col))\n",
    "\n",
    "        # Сохранение логарифма вероятности выбранного действия\n",
    "        log_prob = torch.log(probabilities[0, row, col])\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Сохранение вознаграждения\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        agent.player *=-1\n",
    "        state = next_state\n",
    "\n",
    "    # Вычисление возврата (return) для каждого шага\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns)\n",
    "\n",
    "    # Нормализация возвратов\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "    # Вычисление функции потерь\n",
    "    loss = torch.stack([-log_prob * G for log_prob, G in zip(log_probs, returns)]).sum()\n",
    "\n",
    "    # Обновление параметров модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 10000 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Loss: -3.187407970428467\n",
      "Episode 100, Total Loss: -0.8698832988739014\n",
      "Episode 200, Total Loss: -2.6861014366149902\n",
      "Episode 300, Total Loss: -1.2168773412704468\n",
      "Episode 400, Total Loss: -3.3630428314208984\n",
      "Episode 500, Total Loss: -2.005173921585083\n",
      "Episode 600, Total Loss: 0.9468182325363159\n",
      "Episode 700, Total Loss: -1.5972402095794678\n",
      "Episode 800, Total Loss: -0.8824136257171631\n",
      "Episode 900, Total Loss: -2.9883503913879395\n",
      "Episode 1000, Total Loss: -1.6284780502319336\n",
      "Episode 1100, Total Loss: -4.24084997177124\n",
      "Episode 1200, Total Loss: -0.32937902212142944\n",
      "Episode 1300, Total Loss: -0.38589954376220703\n",
      "Episode 1400, Total Loss: -0.02849733829498291\n",
      "Episode 1500, Total Loss: -2.030236005783081\n",
      "Episode 1600, Total Loss: -0.44572097063064575\n",
      "Episode 1700, Total Loss: -2.7869391441345215\n",
      "Episode 1800, Total Loss: -0.9471529722213745\n",
      "Episode 1900, Total Loss: -0.33421140909194946\n",
      "Episode 2000, Total Loss: -1.6114428043365479\n",
      "Episode 2100, Total Loss: -0.5644298791885376\n",
      "Episode 2200, Total Loss: -0.686021089553833\n",
      "Episode 2300, Total Loss: -0.41282927989959717\n",
      "Episode 2400, Total Loss: -0.3719934821128845\n",
      "Episode 2500, Total Loss: -0.9060654044151306\n",
      "Episode 2600, Total Loss: -0.06807068735361099\n",
      "Episode 2700, Total Loss: -0.35911083221435547\n",
      "Episode 2800, Total Loss: -0.04662812501192093\n",
      "Episode 2900, Total Loss: -0.9818680286407471\n",
      "Episode 3000, Total Loss: -0.029030438512563705\n",
      "Episode 3100, Total Loss: -0.027857495471835136\n",
      "Episode 3200, Total Loss: -0.03577154874801636\n",
      "Episode 3300, Total Loss: -0.8088540434837341\n",
      "Episode 3400, Total Loss: -0.16421279311180115\n",
      "Episode 3500, Total Loss: -0.12100066989660263\n",
      "Episode 3600, Total Loss: -0.13661830127239227\n",
      "Episode 3700, Total Loss: -0.26978278160095215\n",
      "Episode 3800, Total Loss: -0.21785995364189148\n",
      "Episode 3900, Total Loss: -0.18372835218906403\n",
      "Episode 4000, Total Loss: -1.6774109601974487\n",
      "Episode 4100, Total Loss: -0.22951745986938477\n",
      "Episode 4200, Total Loss: -1.7543692588806152\n",
      "Episode 4300, Total Loss: -1.0832483768463135\n",
      "Episode 4400, Total Loss: -2.5568227767944336\n",
      "Episode 4500, Total Loss: -0.5344992280006409\n",
      "Episode 4600, Total Loss: -1.9515973329544067\n",
      "Episode 4700, Total Loss: -1.4204622507095337\n",
      "Episode 4800, Total Loss: -0.15517805516719818\n",
      "Episode 4900, Total Loss: -0.2865244746208191\n",
      "Episode 5000, Total Loss: -2.6730000972747803\n",
      "Episode 5100, Total Loss: -4.50843620300293\n",
      "Episode 5200, Total Loss: -0.5878399014472961\n",
      "Episode 5300, Total Loss: -1.357822060585022\n",
      "Episode 5400, Total Loss: -0.003037565853446722\n",
      "Episode 5500, Total Loss: -2.8662731647491455\n",
      "Episode 5600, Total Loss: -0.212931826710701\n",
      "Episode 5700, Total Loss: -0.8961973190307617\n",
      "Episode 5800, Total Loss: -2.4118802547454834\n",
      "Episode 5900, Total Loss: -3.0598907470703125\n",
      "Episode 6000, Total Loss: -1.2842066287994385\n",
      "Episode 6100, Total Loss: -0.056311994791030884\n",
      "Episode 6200, Total Loss: -0.5270304083824158\n",
      "Episode 6300, Total Loss: -1.0863032341003418\n",
      "Episode 6400, Total Loss: -0.7827180027961731\n",
      "Episode 6500, Total Loss: -0.07742375135421753\n",
      "Episode 6600, Total Loss: -0.05270816758275032\n",
      "Episode 6700, Total Loss: -0.1867237538099289\n",
      "Episode 6800, Total Loss: -0.30437740683555603\n",
      "Episode 6900, Total Loss: -0.019117487594485283\n",
      "Episode 7000, Total Loss: -0.5564305782318115\n",
      "Episode 7100, Total Loss: -1.289368987083435\n",
      "Episode 7200, Total Loss: -0.37609606981277466\n",
      "Episode 7300, Total Loss: -0.24653419852256775\n",
      "Episode 7400, Total Loss: -0.00965965911746025\n",
      "Episode 7500, Total Loss: -0.15869536995887756\n",
      "Episode 7600, Total Loss: -0.29915618896484375\n",
      "Episode 7700, Total Loss: -0.08937172591686249\n",
      "Episode 7800, Total Loss: -0.019952377304434776\n",
      "Episode 7900, Total Loss: -0.057834893465042114\n",
      "Episode 8000, Total Loss: -0.03870335966348648\n",
      "Episode 8100, Total Loss: -0.3283231556415558\n",
      "Episode 8200, Total Loss: -0.3037562966346741\n",
      "Episode 8300, Total Loss: -0.17454060912132263\n",
      "Episode 8400, Total Loss: -0.9042510986328125\n",
      "Episode 8500, Total Loss: -0.8998270630836487\n",
      "Episode 8600, Total Loss: -0.8770155906677246\n",
      "Episode 8700, Total Loss: -0.8470024466514587\n",
      "Episode 8800, Total Loss: -0.03860700502991676\n",
      "Episode 8900, Total Loss: -2.3004961013793945\n",
      "Episode 9000, Total Loss: -0.1989118903875351\n",
      "Episode 9100, Total Loss: -0.5084478259086609\n",
      "Episode 9200, Total Loss: -0.20409756898880005\n",
      "Episode 9300, Total Loss: -0.5645327568054199\n",
      "Episode 9400, Total Loss: -1.8417348861694336\n",
      "Episode 9500, Total Loss: -0.8686037063598633\n",
      "Episode 9600, Total Loss: -1.0363675355911255\n",
      "Episode 9700, Total Loss: -0.7836946249008179\n",
      "Episode 9800, Total Loss: -0.2236381322145462\n",
      "Episode 9900, Total Loss: -1.1285252571105957\n",
      "Episode 10000, Total Loss: -0.18168629705905914\n",
      "Episode 10100, Total Loss: -0.1975153386592865\n",
      "Episode 10200, Total Loss: -0.09696774184703827\n",
      "Episode 10300, Total Loss: -0.04061892256140709\n",
      "Episode 10400, Total Loss: -0.29101768136024475\n",
      "Episode 10500, Total Loss: -0.4240012764930725\n",
      "Episode 10600, Total Loss: -0.4389590620994568\n",
      "Episode 10700, Total Loss: -0.1406002938747406\n",
      "Episode 10800, Total Loss: -0.2195223718881607\n",
      "Episode 10900, Total Loss: -0.626778244972229\n",
      "Episode 11000, Total Loss: -0.12774504721164703\n",
      "Episode 11100, Total Loss: -0.20776179432868958\n",
      "Episode 11200, Total Loss: -1.267001748085022\n",
      "Episode 11300, Total Loss: -0.7467530369758606\n",
      "Episode 11400, Total Loss: -0.8133115172386169\n",
      "Episode 11500, Total Loss: -0.2345082312822342\n",
      "Episode 11600, Total Loss: -0.06341393291950226\n",
      "Episode 11700, Total Loss: -5.1600847244262695\n",
      "Episode 11800, Total Loss: -5.169354438781738\n",
      "Episode 11900, Total Loss: -0.2854435443878174\n",
      "Episode 12000, Total Loss: -0.4914514720439911\n",
      "Episode 12100, Total Loss: -0.3384073078632355\n",
      "Episode 12200, Total Loss: -0.3621421456336975\n",
      "Episode 12300, Total Loss: -0.8294236063957214\n",
      "Episode 12400, Total Loss: -0.036261431872844696\n",
      "Episode 12500, Total Loss: -0.2531060576438904\n",
      "Episode 12600, Total Loss: -1.0753668546676636\n",
      "Episode 12700, Total Loss: -0.0013814008561894298\n",
      "Episode 12800, Total Loss: -0.11596772819757462\n",
      "Episode 12900, Total Loss: -0.03194659203290939\n",
      "Episode 13000, Total Loss: -0.007750374730676413\n",
      "Episode 13100, Total Loss: -1.1104358434677124\n",
      "Episode 13200, Total Loss: -0.5095525979995728\n",
      "Episode 13300, Total Loss: -0.00728751253336668\n",
      "Episode 13400, Total Loss: -0.41720664501190186\n",
      "Episode 13500, Total Loss: -0.031957197934389114\n",
      "Episode 13600, Total Loss: -0.0776156634092331\n",
      "Episode 13700, Total Loss: -3.362156629562378\n",
      "Episode 13800, Total Loss: -0.03576565533876419\n",
      "Episode 13900, Total Loss: -0.6025252938270569\n",
      "Episode 14000, Total Loss: -0.0047543467953801155\n",
      "Episode 14100, Total Loss: -0.0007498892373405397\n",
      "Episode 14200, Total Loss: -0.15953779220581055\n",
      "Episode 14300, Total Loss: -0.33858054876327515\n",
      "Episode 14400, Total Loss: -0.5198767185211182\n",
      "Episode 14500, Total Loss: -0.2823418080806732\n",
      "Episode 14600, Total Loss: -1.134334921836853\n",
      "Episode 14700, Total Loss: -0.08588368445634842\n",
      "Episode 14800, Total Loss: -0.9479066133499146\n",
      "Episode 14900, Total Loss: -0.2236124575138092\n",
      "Episode 15000, Total Loss: -0.05777589976787567\n",
      "Episode 15100, Total Loss: -1.5113608837127686\n",
      "Episode 15200, Total Loss: -0.000705127022229135\n",
      "Episode 15300, Total Loss: -0.0002034864155575633\n",
      "Episode 15400, Total Loss: -0.5122931003570557\n",
      "Episode 15500, Total Loss: -0.0004390993854030967\n",
      "Episode 15600, Total Loss: -0.014737052842974663\n",
      "Episode 15700, Total Loss: -0.004015983548015356\n",
      "Episode 15800, Total Loss: -0.004400181118398905\n",
      "Episode 15900, Total Loss: -0.011187050491571426\n",
      "Episode 16000, Total Loss: -0.004827999509871006\n",
      "Episode 16100, Total Loss: -0.00022332926164381206\n",
      "Episode 16200, Total Loss: -0.002072388306260109\n",
      "Episode 16300, Total Loss: -0.0019184831762686372\n",
      "Episode 16400, Total Loss: -0.05914459004998207\n",
      "Episode 16500, Total Loss: -0.004847219213843346\n",
      "Episode 16600, Total Loss: -0.11583159863948822\n",
      "Episode 16700, Total Loss: -0.15835785865783691\n",
      "Episode 16800, Total Loss: -0.09614279866218567\n",
      "Episode 16900, Total Loss: -0.010650118812918663\n",
      "Episode 17000, Total Loss: -2.287991762161255\n",
      "Episode 17100, Total Loss: -0.0006371745839715004\n",
      "Episode 17200, Total Loss: 0.002767696511000395\n",
      "Episode 17300, Total Loss: -0.010438787750899792\n",
      "Episode 17400, Total Loss: -0.0042117866687476635\n",
      "Episode 17500, Total Loss: -0.18450340628623962\n",
      "Episode 17600, Total Loss: -0.041290801018476486\n",
      "Episode 17700, Total Loss: -2.3581390380859375\n",
      "Episode 17800, Total Loss: -0.02128499001264572\n",
      "Episode 17900, Total Loss: -0.012197151780128479\n",
      "Episode 18000, Total Loss: -0.08291206508874893\n",
      "Episode 18100, Total Loss: -0.004809640813618898\n",
      "Episode 18200, Total Loss: 8.97503923624754e-06\n",
      "Episode 18300, Total Loss: -0.0009376692469231784\n",
      "Episode 18400, Total Loss: -0.12122133374214172\n",
      "Episode 18500, Total Loss: -0.0023611681535840034\n",
      "Episode 18600, Total Loss: -0.000631389208137989\n",
      "Episode 18700, Total Loss: 0.0008500376134179533\n",
      "Episode 18800, Total Loss: -0.3761748671531677\n",
      "Episode 18900, Total Loss: -0.00012962623441126198\n",
      "Episode 19000, Total Loss: -0.000470331113319844\n",
      "Episode 19100, Total Loss: -0.03588016703724861\n",
      "Episode 19200, Total Loss: -0.0012918615248054266\n",
      "Episode 19300, Total Loss: -2.0525097846984863\n",
      "Episode 19400, Total Loss: -3.714972734451294\n",
      "Episode 19500, Total Loss: -0.44582849740982056\n",
      "Episode 19600, Total Loss: -0.00026364499353803694\n",
      "Episode 19700, Total Loss: -0.06360949575901031\n",
      "Episode 19800, Total Loss: -0.0050908038392663\n",
      "Episode 19900, Total Loss: -0.0018011170905083418\n",
      "Episode 20000, Total Loss: -0.0318758487701416\n",
      "Episode 20100, Total Loss: -0.9077272415161133\n",
      "Episode 20200, Total Loss: -1.9447044134140015\n",
      "Episode 20300, Total Loss: -0.0003984706709161401\n",
      "Episode 20400, Total Loss: -0.011239059269428253\n",
      "Episode 20500, Total Loss: -0.012523853220045567\n",
      "Episode 20600, Total Loss: -0.005339358933269978\n",
      "Episode 20700, Total Loss: -0.11227374523878098\n",
      "Episode 20800, Total Loss: -0.000625965534709394\n",
      "Episode 20900, Total Loss: -0.7850760817527771\n",
      "Episode 21000, Total Loss: -0.004450913984328508\n",
      "Episode 21100, Total Loss: -0.006925089284777641\n",
      "Episode 21200, Total Loss: -0.03343459218740463\n",
      "Episode 21300, Total Loss: -0.005523563828319311\n",
      "Episode 21400, Total Loss: -0.004369506612420082\n",
      "Episode 21500, Total Loss: -0.05278153344988823\n",
      "Episode 21600, Total Loss: -1.378018856048584\n",
      "Episode 21700, Total Loss: -0.04127408564090729\n",
      "Episode 21800, Total Loss: -0.002109617693349719\n",
      "Episode 21900, Total Loss: -0.9875316619873047\n",
      "Episode 22000, Total Loss: -0.5723584890365601\n",
      "Episode 22100, Total Loss: -0.32961517572402954\n",
      "Episode 22200, Total Loss: -0.7058711647987366\n",
      "Episode 22300, Total Loss: -0.1462428718805313\n",
      "Episode 22400, Total Loss: -0.02463618665933609\n",
      "Episode 22500, Total Loss: -0.5197399854660034\n",
      "Episode 22600, Total Loss: -0.03645555302500725\n",
      "Episode 22700, Total Loss: -0.04945714399218559\n",
      "Episode 22800, Total Loss: -0.678462564945221\n",
      "Episode 22900, Total Loss: -0.056757405400276184\n",
      "Episode 23000, Total Loss: -0.112820103764534\n",
      "Episode 23100, Total Loss: -0.05596862733364105\n",
      "Episode 23200, Total Loss: -0.12766528129577637\n",
      "Episode 23300, Total Loss: -0.03637611120939255\n",
      "Episode 23400, Total Loss: -0.03606821596622467\n",
      "Episode 23500, Total Loss: -0.5909184217453003\n",
      "Episode 23600, Total Loss: -0.1647057980298996\n",
      "Episode 23700, Total Loss: -0.2850615084171295\n",
      "Episode 23800, Total Loss: -1.061467170715332\n",
      "Episode 23900, Total Loss: -0.11023188382387161\n",
      "Episode 24000, Total Loss: -2.500326633453369\n",
      "Episode 24100, Total Loss: -0.06558330357074738\n",
      "Episode 24200, Total Loss: -0.15978378057479858\n",
      "Episode 24300, Total Loss: -1.1624202728271484\n",
      "Episode 24400, Total Loss: -0.0790446549654007\n",
      "Episode 24500, Total Loss: -1.128365159034729\n",
      "Episode 24600, Total Loss: -0.3631635904312134\n",
      "Episode 24700, Total Loss: -0.002860401989892125\n",
      "Episode 24800, Total Loss: -3.5311968326568604\n",
      "Episode 24900, Total Loss: -0.3889060318470001\n",
      "Episode 25000, Total Loss: -0.0005025821155868471\n",
      "Episode 25100, Total Loss: -0.7841309905052185\n",
      "Episode 25200, Total Loss: -0.3004244267940521\n",
      "Episode 25300, Total Loss: -0.07243601977825165\n",
      "Episode 25400, Total Loss: -0.11503783613443375\n",
      "Episode 25500, Total Loss: -0.8596872091293335\n",
      "Episode 25600, Total Loss: -0.18117982149124146\n",
      "Episode 25700, Total Loss: -1.6491059064865112\n",
      "Episode 25800, Total Loss: -0.0026635336689651012\n",
      "Episode 25900, Total Loss: -0.0011528939940035343\n",
      "Episode 26000, Total Loss: -0.11438150703907013\n",
      "Episode 26100, Total Loss: -0.41952401399612427\n",
      "Episode 26200, Total Loss: -0.23156143724918365\n",
      "Episode 26300, Total Loss: -0.02985348552465439\n",
      "Episode 26400, Total Loss: -0.012060219421982765\n",
      "Episode 26500, Total Loss: -0.011067969724535942\n",
      "Episode 26600, Total Loss: -0.04257683828473091\n",
      "Episode 26700, Total Loss: -0.0053212083876132965\n",
      "Episode 26800, Total Loss: -0.49469369649887085\n",
      "Episode 26900, Total Loss: -0.016247661784291267\n",
      "Episode 27000, Total Loss: -3.228369951248169\n",
      "Episode 27100, Total Loss: -0.03749573230743408\n",
      "Episode 27200, Total Loss: -0.43702325224876404\n",
      "Episode 27300, Total Loss: -0.2810976803302765\n",
      "Episode 27400, Total Loss: -0.0008248360245488584\n",
      "Episode 27500, Total Loss: -0.21720850467681885\n",
      "Episode 27600, Total Loss: -0.004212493076920509\n",
      "Episode 27700, Total Loss: -0.1480131447315216\n",
      "Episode 27800, Total Loss: -0.00302663235925138\n",
      "Episode 27900, Total Loss: 0.2677029073238373\n",
      "Episode 28000, Total Loss: -0.0931403711438179\n",
      "Episode 28100, Total Loss: -0.01530888769775629\n",
      "Episode 28200, Total Loss: -2.018192768096924\n",
      "Episode 28300, Total Loss: -0.008660359308123589\n",
      "Episode 28400, Total Loss: -0.06381839513778687\n",
      "Episode 28500, Total Loss: -0.024983469396829605\n",
      "Episode 28600, Total Loss: -0.3069886565208435\n",
      "Episode 28700, Total Loss: -0.01022112276405096\n",
      "Episode 28800, Total Loss: -0.011513768695294857\n",
      "Episode 28900, Total Loss: -0.09093266725540161\n",
      "Episode 29000, Total Loss: -0.017995689064264297\n",
      "Episode 29100, Total Loss: -0.0458793118596077\n",
      "Episode 29200, Total Loss: -0.019896844401955605\n",
      "Episode 29300, Total Loss: 0.004753468558192253\n",
      "Episode 29400, Total Loss: -0.08957892656326294\n",
      "Episode 29500, Total Loss: -0.7891258597373962\n",
      "Episode 29600, Total Loss: -1.0192567110061646\n",
      "Episode 29700, Total Loss: -0.00614219019189477\n",
      "Episode 29800, Total Loss: -0.6198989748954773\n",
      "Episode 29900, Total Loss: 0.041999075561761856\n",
      "Episode 30000, Total Loss: -0.08063459396362305\n",
      "Episode 30100, Total Loss: -0.18983812630176544\n",
      "Episode 30200, Total Loss: -1.211142897605896\n",
      "Episode 30300, Total Loss: -0.0008115211385302246\n",
      "Episode 30400, Total Loss: -0.0011590559734031558\n",
      "Episode 30500, Total Loss: -0.0005685082287527621\n",
      "Episode 30600, Total Loss: -0.3571780323982239\n",
      "Episode 30700, Total Loss: 0.0005181768792681396\n",
      "Episode 30800, Total Loss: -0.06353389471769333\n",
      "Episode 30900, Total Loss: -0.02282242849469185\n",
      "Episode 31000, Total Loss: -0.0004247145261615515\n",
      "Episode 31100, Total Loss: 8.711225382285193e-05\n",
      "Episode 31200, Total Loss: -0.07989329844713211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Получение вероятностей для текущего игрока\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     agent\u001b[38;5;241m.\u001b[39mplayer \u001b[38;5;241m=\u001b[39m current_player  \u001b[38;5;66;03m# Задаем игрока (1 или -1)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_move(probabilities)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Выполнение действия в среде\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\pip-torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\pip-torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mTicTacToeAgent.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Первый сверточный блок\u001b[39;00m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)), negative_slope\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Второй сверточный блок\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\pip-torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 2 версия\n",
    "\n",
    "agent = TicTacToeAgent()\n",
    "optimizer = optim.AdamW(agent.parameters(), lr=0.001)\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "num_episodes = 200000\n",
    "gamma = 0.99  # Коэффициент дисконтирования\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    log_probs_player1 = []\n",
    "    log_probs_player2 = []\n",
    "    rewards_player1 = []\n",
    "    rewards_player2 = []\n",
    "\n",
    "    done = False\n",
    "    current_player = 1  # Начинаем с игрока 1\n",
    "\n",
    "    while not done:\n",
    "        # Получение вероятностей для текущего игрока\n",
    "        agent.player = current_player  # Задаем игрока (1 или -1)\n",
    "        probabilities = agent(state)\n",
    "        row, col = agent.select_move(probabilities)\n",
    "\n",
    "        # Выполнение действия в среде\n",
    "        next_state, reward, done = env.step((row, col))\n",
    "\n",
    "        # Сохранение логарифма вероятности действия и награды\n",
    "        log_prob = torch.log(probabilities[0, row, col])\n",
    "        if current_player == 1:\n",
    "            log_probs_player1.append(log_prob)\n",
    "            rewards_player1.append(reward)\n",
    "        else:\n",
    "            log_probs_player2.append(log_prob)\n",
    "            rewards_player2.append(-reward)  # Инверсия награды для второго игрока\n",
    "\n",
    "        state = next_state\n",
    "        current_player *= -1  # Смена игрока\n",
    "\n",
    "    # Вычисление возвратов отдельно для каждого игрока\n",
    "    def compute_returns(rewards, gamma):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    returns_player1 = compute_returns(rewards_player1, gamma)\n",
    "    returns_player2 = compute_returns(rewards_player2, gamma)\n",
    "\n",
    "    # Нормализация возвратов\n",
    "    returns_player1 = (returns_player1 - returns_player1.mean()) / (returns_player1.std() + 1e-8)\n",
    "    returns_player2 = (returns_player2 - returns_player2.mean()) / (returns_player2.std() + 1e-8)\n",
    "\n",
    "    # Вычисление потерь отдельно для каждого игрока\n",
    "    loss_player1 = torch.stack([-log_prob * G for log_prob, G in zip(log_probs_player1, returns_player1)]).sum()\n",
    "    loss_player2 = torch.stack([-log_prob * G for log_prob, G in zip(log_probs_player2, returns_player2)]).sum()\n",
    "\n",
    "    # Итоговая функция потерь\n",
    "    total_loss = loss_player1 + loss_player2\n",
    "\n",
    "    # Обновление параметров модели\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Loss: {total_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 9.3382e-10],\n",
       "         [0.0000e+00, 3.4946e-12, 0.0000e+00]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.player = 1\n",
    "input_tensor = torch.tensor([[[[1.,  0,   1.], \n",
    "                               [-1., 1.,  0.], \n",
    "                               [-1.,  0.,  -1.]]]])\n",
    "# input_tensor = torch.zeros(1, 1, 3, 3)  # Входная матрица 3x3\n",
    "agent.eval()\n",
    "output = agent(input_tensor)\n",
    "print(agent.select_move(output))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.player"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
